{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3453d723-9151-416b-b532-92ec887a918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f20bab9-9856-4000-86d9-84743149d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results_tblout_output(file_fullpath):\n",
    "    \"\"\"\n",
    "    Parse results tblout output to extract key information and store it in a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_fullpath (str): Full path to the file containing results tblout output.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing parsed hit data with cleaned and formatted columns.\n",
    "    \"\"\"\n",
    "    # Precompile the regex pattern to remove accession version numbers.\n",
    "    accession_pattern = re.compile(r'_[0-9]+$')\n",
    "\n",
    "    # Initialize an empty list to store hit data.\n",
    "    hits = []\n",
    "\n",
    "    # Read the file, skipping the first three header lines.\n",
    "    with open(file_fullpath, 'r') as handle:\n",
    "        lines = handle.readlines()[3:]\n",
    "\n",
    "        # Process each line in the file.\n",
    "    for line in lines:\n",
    "        if not line.strip() or line.startswith('#'):\n",
    "            continue  # Skip empty lines and comments.\n",
    "        \n",
    "        # Split the line into columns and remove empty strings.\n",
    "        cols = [x for x in line.strip().split() if x]\n",
    "\n",
    "        # Extract the Profile name and format it.\n",
    "        # Profile_text = \"Nuo\" + cols[2].split('_')[3].replace('nuo', '').upper()\n",
    "\n",
    "        # Create a dictionary for the current hit.\n",
    "        hit = {\n",
    "            'Accession': accession_pattern.sub('', cols[0]),  # Clean the accession number.\n",
    "            'ProteinAccession' : cols[0],\n",
    "            'Profile': cols[2],\n",
    "            'evalue': np.float64(cols[4]),\n",
    "            'BitScore': np.float64(cols[5]),\n",
    "            'Bias': np.float64(cols[6]),\n",
    "            # 'evalueDomain': float(cols[7]),\n",
    "            # 'BitScoreDomain': float(cols[8]),\n",
    "            # 'BiasDomain': float(cols[9]),\n",
    "            'SequenceDesc': \" \".join(cols[18:])  # Combine remaining columns for description.\n",
    "        }\n",
    "\n",
    "        # Append the hit to the list.\n",
    "        hits.append(hit)\n",
    "\n",
    "    # Convert the list of hits to a DataFrame and return.\n",
    "    return pd.DataFrame(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34e0231-8722-45c1-9cbc-2485a97322ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hmmer_results(result_dir, pattern_str=r'#\\s*(\\d+)\\s*#\\s*(\\d+)\\s*'):\n",
    "    \"\"\"\n",
    "    Processes HMMER output files from the specified directory, extracts relevant data, \n",
    "    performs log transformation on e-values, and splits the 'Profile' column.\n",
    "\n",
    "    Parameters:\n",
    "    result_dir (str): The directory containing the .txt HMMER output files.\n",
    "    pattern_str (str): The regex pattern for extracting 'Start' and 'End' from 'SequenceDesc'. \n",
    "                       Defaults to '#\\\\s*(\\\\d+)\\\\s*#\\\\s*(\\\\d+)\\\\s*'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated and processed DataFrame containing the results from all files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load all dataframes from files\n",
    "    file_paths = glob(os.path.join(result_dir, '*.txt'))  # Get all .txt files in directory\n",
    "    all_dataframes = [parse_results_tblout_output(file) for file in tqdm(file_paths)]\n",
    "\n",
    "    # Concatenate all dataframes into a single one\n",
    "    results = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # Sort by 'evalue' and remove duplicatess\n",
    "    results.sort_values(by='evalue', inplace=True)\n",
    "    results.drop_duplicates(subset=['Accession', 'ProteinAccession'], keep='first', inplace=True)\n",
    "\n",
    "    # Split the 'Profile' column into three new columns: Subunit, SeqsClustThreshold, HMMParameter\n",
    "    results[['Subunit', 'SeqsClustThreshold', 'HMMParameter']] = results['Profile'].str.split('_', expand=True)\n",
    "\n",
    "    # Extract 'Start' and 'End' from 'SequenceDesc' using regex\n",
    "    pattern = re.compile(pattern_str)\n",
    "    matches = results['SequenceDesc'].apply(lambda x: pattern.search(x) if pd.notnull(x) else None)\n",
    "\n",
    "    # Assign 'Start' and 'End' based on regex matches\n",
    "    results['Start'] = matches.apply(lambda m: int(m.group(1)) if m else 0)\n",
    "    results['End'] = matches.apply(lambda m: int(m.group(2)) if m else 0)\n",
    "\n",
    "    # Apply log10 transformation to the 'evalue' column\n",
    "    results['log10evalue'] = np.log10(results['evalue'])\n",
    "\n",
    "    # Remove the 'Profile' column, as it's no longer needed\n",
    "    results.drop(columns=['Profile', 'SeqsClustThreshold', 'HMMParameter'], inplace=True)\n",
    "\n",
    "    # Reset index after all modifications\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4a11af-1e00-4428-b303-3134595510b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47278/47278 [00:17<00:00, 2723.56it/s]\n"
     ]
    }
   ],
   "source": [
    "results = process_hmmer_results(\"/Users/akshayonly/Work/04-Complex-I/Data/04-HMM-Analysis-Data/01-HMM-Subunits-Search-Raws\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8550c4-ab19-4fa4-9912-2ae2627b4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('/Users/akshayonly/Work/04-Complex-I/Data/07-Figures/regenerate/results_unfil.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
